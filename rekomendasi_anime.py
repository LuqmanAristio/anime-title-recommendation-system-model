# -*- coding: utf-8 -*-
"""Rekomendasi Anime.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17APB2ATEiZGpzYzvVjoV96my-4W_KR2a

# **Import Library dan Dataset dari Drive**
"""

from tensorflow.keras import layers
from google.colab import drive
from tensorflow import keras
from zipfile import ZipFile

import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sns
import pandas as pd
import numpy as np
import zipfile,os
import re

drive.mount('/content/drive')

"""# **Data Loading**

Dataset dapat diunduh pada website Kaggle dengan link berikut ini : https://www.kaggle.com/datasets/CooperUnion/anime-recommendations-database
"""

# melakukan ekstraksi pada file dataset
local_zip = '/content/drive/MyDrive/anime_dataset.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/tmp')
zip_ref.close()

"""Terdapat 2 file utama dalam dataset tersebut dalam format CSV. Pertama yakni anime.csv yang berisi informasi tentang anime yang berasal dari https://myanimelist.net/. Kemudian rating.csv merupakan rating yang diberikan user kepada anime yang ditonton olehnya."""

# mengambil dataset
anime = pd.read_csv("/tmp/anime.csv")
rating = pd.read_csv("/tmp/rating.csv")

"""# **Data Understanding and EDA**

### **anime.csv**
"""

anime.info()

anime.head(10)

"""Terdapat 12.294 sampel atau judul anime dalam dataset tersebut dengan 7 kolom atau fitur. Komposisinya yakni 3 fitur numerik dan 4 fitur kategorikal. Pada fitur rating, nilai diperoleh berdasarkan hasil rata rata dari keseluruhan user yang menilai anime tersebut dengan maksimal nilai yaitu 10. Genre sebuah anime tidak terbatas pada 1 genre saja sehingga terdapat anime yang memiliki banyak genre bahkan hingga lebih dari 5. Terdapat juga type yang merupakan saluran tayang dari anime tersebut."""

anime.describe()

"""Rating yang diberikan dimulai dari 1 yang terkecil hingga maksimal yaitu 10."""

print('Banyak jenis genre : ', len(anime.genre.unique()))
print('Jeni-jenis genre : ', anime.genre.unique())

"""Bisa dilihat terdapat banyak sekali genre dalam dataset tersebut. Hal itu dikarenakan genre tersebut tidak ditampilkan secara terpisah, sehingga sering terjadinya pengulangan genre pada jenis anime yang memiliki lebih dari 1 genre. Contoh pada anime Kimi No Nawa dengan 4 genre. Pada tahapan pemrosesan data nantinya genre akan dipisahkan."""

type_count = anime['type'].value_counts()

sns.barplot(x=type_count.values,
            y=type_count.index,
            palette='muted').set_title('Tipe Penayangan Anime')

plt.tight_layout()
plt.show()

"""Tipe penayangan anime terbesar dipegang oleh TV dengan lebih dari 3000 judul anime kemudian diikuti oleh OVA dan Movie.

### **rating.csv**
"""

rating.info()
rating

"""Pada dataset rating, jumlah sampel berukuran cukup besar yakni sekitar 7 juta sampel. Hal itu dikarenakan terdapat rating -1 yang merupakan tanda bahwa user sudah menonton anime tersebut namun tidak memberikan rating. User yang sudah menonton dapat memberikan rating dari 1-10 atau tidak memberikan rating."""

print('Jumlah user : ', len(rating.user_id.unique()))
print('Jumlah anime yang diberi rating oleh user : ', (rating['rating'] != -1).sum())

rating.isnull().sum()

"""Setelah dicek, ternyata hanya terdapat 73 ribu user unik dalam 7 juta sampel tersebut. Kemudian juga jumlah rating hanya terdapat kurang lebih 6 juta saja sehingga dapat dikatakan terdapat 1 juta lebih user yang tidak memberikan rating pada anime yang sudah ditontonnya.

# **Data Preprocessing**

Bisa dilihat sebelumnya pada dataset anime.csv bahwa banyak sekali kesalahan penulisan atau simbol simbol tidak penting pada fitur name. Kemudian juga pada fitur genre terdapat tanda koma untuk memisahkan setiap genre. Hal tersebut dapat menyebabkan error pada saat kita memprosesnya dikarenakan dalam beberapa proses, tanda koma bisa dianggap sebagai tipe numerik atau float. 

Oleh karena itu perlu dilakukan data cleaning terlebih dahulu
"""

def name_cleaning(text):
    text = re.sub(r'&quot;', '', text)
    text = re.sub(r'&quot;', '', text)
    text = re.sub(r'.hack//', '', text)
    text = re.sub(r'&#039;', '', text)
    text = re.sub(r'A&#039;s', '', text)
    text = re.sub(r'I&#039;', 'I\'', text)
    text = re.sub(r'&amp;', 'and', text)
    text = re.sub(r'Â°', 'and', text)
    return text

def genre_cleaning(text):
    text = re.sub(r",","", str(text))
    return text

anime['name'] = anime['name'].apply(name_cleaning)
anime['genre'] = anime['genre'].apply(genre_cleaning)

anime.info()
anime.head(5)

"""Bisa dilihat pada hasil diatas, pada fitur name sudah tidak terdapat simbol atau angka yang mengganggu kemudian pada fitur genre sudah tidak terdapat koma sebagai pemisah antar genre.

Berikutnya yakni menggabungkan kedua dataset tersebut, karena terdapat kesamaan nama pada fitur rating kedua dataset, maka fitur rating pada dataset kedua kita ubah namanya menjadi rating_user agar tampak berbeda.
"""

# menggabungkan 2 dataset menjadi 1 berdasarkan anime id dan mengurutkannya
df = pd.merge(anime,rating,on='anime_id',suffixes= ['', '_user'])
df = df.sort_values('user_id', ascending=True)

# menghapus rating -1 karena tidak relevan
df.drop(df[df['rating_user'] == -1].index, inplace = True)

# cek dan hapus nilai null
df = df.dropna()
df.isnull().sum()

"""Kemudian pada fitur rating user, terdapat nilai -1 yang menandakan bahwa user tersebut belum memberi rating pada animenya. Informasi tersebut tidak kita perlukan saat ini, sehingga lebih baik kita hapus saja.

Setelah dilihat, saya rasa kita tidak memerlukan fitur type, episode dan members saat ini. Oleh karena itu lebih baik kita drop saja agar mempercepat proses nantinya.
"""

df.drop(['type', 'members','episodes'], axis=1)
df

"""# **Data Preparation**"""

# dataset yang akan dipakai
df

# menampilkan genre secara unik
df.genre.unique()

"""Genre anime bisa lebih dari 1 oleh karena itu terdapat cukup banyak genre karena kondisi masih belum dipisah."""

# mengurutkan data berdasarkan anime id
df.sort_values('anime_id')

# menghapus duplikat untuk proses training model
preparation = df.drop_duplicates('anime_id')
preparation

"""Saat proses pelatihan model, data duplikat tidak diperlukan dalam pemrosesan. Oleh karena itu lebih baik dihapus saja. Selanjutnya kita akan mengubah data dalam bentuk list agar dengan ukuran yang sama agar dapat dijadikan dictionary"""

# Mengonversi data series anime id menjadi dalam bentuk list
anime_id = preparation['anime_id'].tolist()
 
# Mengonversi data series name menjadi dalam bentuk list
anime_name = preparation['name'].tolist()
 
# Mengonversi data series genre menjadi dalam bentuk list
anime_genre = preparation['genre'].tolist()
 
print(len(anime_id))
print(len(anime_name))
print(len(anime_genre))

# Membuat dictionary
dataset_new = pd.DataFrame({
    'id': anime_id,
    'name': anime_name,
    'genre': anime_genre
})

dataset_new

"""Setelah melewati berbagai tahapan, dataset dengan 3 fitur diatas siap digunakan dalam pemodelan.

# **Model Development**

## **Content Based Filtering**

Pada tahapan ini akan dikembangkan model dengan content based filtering atau dengan singkatnya yakni rekomendasi berdasarkan kesesuaian fiturnya. Kesesuaian yang digunakan disini adalah genre dari anime tersebut. Seperti yang diketahui genre merupakan identitas sebuah anime yang penting diperhatikan.
"""

data = dataset_new

data.sample(5)

"""### **TF-IDF Vectorizer**

Berikutnya yakni tahapan TF IDF, tahapan ini sangat penting dalam content based filtering. Hal tersebut dikarenakan pada tahap inilah kita bisa melihat representasi fitur penting dari genre anime yang akan kita olah.
"""

from sklearn.feature_extraction.text import TfidfVectorizer
 
# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()
 
# Melakukan perhitungan idf pada data genre
tf.fit(data['genre']) 
 
# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data['genre']) 
 
# Melihat ukuran matrix tfidf
tfidf_matrix.shape

"""Bisa dilihat dari hasil tf idf, terdapat 48 genre dari 9924 sampel. Di tahapan sebelumnya, genre unik dalam sampel berjumlah 3000 namun setelah dilakukan mapping tf idf hanya terdeteksi 48 genre saja. Berikutnya yakni mengubahnya dalam bentuk matriks"""

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan genre anime
# Baris diisi dengan nama anime
 
pd.DataFrame(
    tfidf_matrix.todense(), 
    columns=tf.get_feature_names(),
    index=data.name
).sample(22, axis=1).sample(10, axis=0)

"""### **Cosine Similarity**

Pada tahapan berikutnya akan dicari cosine similarity antara anime dengan genrenya. Tujuannya agar kita dapat menemukan kesamaan diantara berbagai anime berdasarkan genre mereka. Kesamaan tersebut diubah dalam bentuk numerik dan disusun menjadi matriks.
"""

from sklearn.metrics.pairwise import cosine_similarity
 
# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix) 
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa nama
cosine_sim_df = pd.DataFrame(cosine_sim, index=data['name'], columns=data['name'])
print('Shape:', cosine_sim_df.shape)
 
# Melihat similarity matrix pada setiap anime
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""Bisa dilihat pada gambar diatas, kita dapat melihat kesamaan antar berbagai anime dengan skor numerik mereka mulai dari 0 hingga 1. Kemudian berikutnya kita tinggal membuat fungsi untuk memprediksinya saja."""

def anime_recommendations(nama_anime, similarity_data=cosine_sim_df, items=data[['name', 'genre']], k=10): 
    # Mengambil data dengan menggunakan argpartition untuk melakukan partisi secara tidak langsung sepanjang sumbu yang diberikan    
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,nama_anime].to_numpy().argpartition(
        range(-1, -k, -1))
    
    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]
    
    # Drop nama anime agar nama anime yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(nama_anime, errors='ignore')

    result = pd.DataFrame(closest).merge(items).head(k)

    return result

"""Berikutnya tinggal memanggil fungsinya dengan menginputkan judul anime yang sudah pernah ditonton"""

# input judul
input = 'Sword Art Online'

rec = data[data.name.eq(input)]
rec

# hasil rekomendasi
hasil = anime_recommendations(input)
hasil

"""### **Metrik Evaluasi**

Setelah berhasil membuat rekomendasi anime, berikutnya yakni mengevaluasi hasil metrik dengan teknik metrik precision. Dalam metrik precision kasus ini, nilai akan diambil berdasarkan genre yang sesuai dengan input user. 

Nilai setiap genre yang sesuai dengan input akan dibagi dengan keseluruhan genre hasil rekomendasi. Contoh pada rekomendasi pertama nilai yang diperoleh adalah 1 dari hasil bagi 5 benar dan 5 total genre. Kemudian pada rekomendasi terakhir hanya 4 genre yang sesuai maka hasilnya adalah 4 / 6 yakni 0.6.

Dari seluruh hasil presisi rekomendasi tersebut, diakhir nanti akan dicari rata-ratanya. Hasil rata rata tersebutlah yang merupakan metrik presisi pada kasus ini
"""

# membuat list dari genre input untuk dibandingkan
x = rec['genre'].tolist()
y = x[0].split()

loop_skor = 0

# perulangan untuk membandingkan genre input dan hasil
for i in range (e):

  q = len(y)
  e = len(hasil)

  temp_skor = 0
  skor = 0

  # membuat list dari genre hasil untuk dibandingkan
  a = hasil._get_value(i, 'genre')
  b = a.split()

  if len(b) < q:
    q = len(b)

  for j in range (q):

    if y[j] == b[j]:
      skor += 1

  temp_skor = skor / len(b)
  loop_skor += temp_skor

# hasil jumlah keseluruhan dibagi total
result = loop_skor/e

print("Skor Presisi Akhir : ", result)

"""## **Collaborative Filtering**

Berbeda dengan content based, pada collaborative filtering ini rekomendasi yang diberikan adalah sesuai dengan komunitas pengguna terkait. Sehingga faktor pengguna lain atau rating berperan besar disini.

### **Data Preparation**
"""

df = rating

#menghapus rating -1 kembali
df.drop(df[df['rating'] == -1].index, inplace = True)

# menghapus data agar tersisa 50 ribu
df.drop(df.index[50000:6337241], inplace=True)

df

"""Pada collaborative filtering disini hanya akan digunakan 50 ribu data saja dari 6 juta data yang ada. Hal itu bertujuan agar saat proses training epoch dapat berjalan lebih cepat. Namun jika ingin mengubahnya dapat ditingkatkan lagi tapi harus ingat dengan spesifikasi komputer

Berikutnya kita akan masuk ke tahap data preprocessing lagi yang bertujuan untuk encode fitur user id dan anime id
"""

# Mengubah user id menjadi list tanpa nilai yang sama
user_ids = df['user_id'].unique().tolist()
 
# Melakukan encoding user id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}

# Melakukan proses encoding angka ke ke user id
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Mengubah anime id menjadi list tanpa nilai yang sama
anime_ids = df['anime_id'].unique().tolist()
 
# Melakukan proses encoding anime id
anime_to_anime_encoded = {x: i for i, x in enumerate(anime_ids)}
 
# Melakukan proses encoding angka ke anime id
anime_encoded_to_anime = {i: x for i, x in enumerate(anime_ids)}

# Mapping user id ke dataframe user
df['user'] = df['user_id'].map(user_to_user_encoded)
 
# Mapping anime id ke dataframe anime
df['anime'] = df['anime_id'].map(anime_to_anime_encoded)

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)
print(num_users)
 
# Mendapatkan jumlah anime
num_anime = len(anime_encoded_to_anime)
print(num_anime)
 
# Mengubah rating menjadi nilai float
df['rating'] = df['rating'].values.astype(np.float32)
 
# Nilai minimum rating
min_rating = min(df['rating'])
 
# Nilai maksimal rating
max_rating = max(df['rating'])
 
print('Number of User: {}, Number of Anime: {}, Min Rating: {}, Max Rating: {}'.format(
    num_users, num_anime, min_rating, max_rating
))

"""Bisa dilihat informasi jumlah user, anime beserta rating min maxnya."""

# Mengacak dataset
df = df.sample(frac=1, random_state=42)
df

"""Berikutnya yakni membagi dataset menjadi 80% data training dan 20% data testing

### **Train Test Split**
"""

# Membuat variabel x untuk mencocokkan data user dan anime menjadi satu value
x = df[['user', 'anime']].values
 
# Membuat variabel y untuk membuat rating dari hasil 
y = df['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
 
# Membagi menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
 
print(x, y)

"""### **Training**

Kemudian masuk ke tahapan utama yakni training, pada model kali ini digunakan teknik embedding untuk menghitung nilai kecocokan user dengan anime yang dicari. Pembangunan model menggunakan keras model class
"""

class RecommenderNet(tf.keras.Model):
 
  # Insialisasi fungsi
  def __init__(self, num_users, num_anime, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_anime = num_anime
    self.embedding_size = embedding_size
    self.user_embedding = layers.Embedding( 
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.user_bias = layers.Embedding(num_users, 1) 
    self.anime_embedding = layers.Embedding( 
        num_anime,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    self.anime_bias = layers.Embedding(num_anime, 1) 
 
  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) 
    user_bias = self.user_bias(inputs[:, 0]) 
    anime_vector = self.anime_embedding(inputs[:, 1])
    anime_bias = self.anime_bias(inputs[:, 1])
 
    dot_user_anime = tf.tensordot(user_vector, anime_vector, 2) 
 
    x = dot_user_anime + user_bias + anime_bias
    
    return tf.nn.sigmoid(x) # activation sigmoid

model = RecommenderNet(num_users, num_anime, 50) # inisialisasi model
 
# model compile
model.compile(
    loss = tf.keras.losses.BinaryCrossentropy(),
    optimizer = keras.optimizers.Adam(learning_rate=0.001),
    metrics=[tf.keras.metrics.RootMeanSquaredError()]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    batch_size = 8,
    epochs = 50,
    validation_data = (x_val, y_val)
)

"""### **Metrik Evaluasi**"""

# ploting hasil training model
plt.plot(history.history['root_mean_squared_error'])
plt.plot(history.history['val_root_mean_squared_error'])
plt.title('model_metrics')
plt.ylabel('root_mean_squared_error')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

"""Bisa dilihat dari hasil matriks diatas bisa dilihat pada data training nilai RMSE terus menurun hingga menyentuh dibawah 0.13. Kemudian pada data testing nilai RMSE stabil antara 0.15 dan 0.14. Hal ini merupakan hasil yang baik mengingat sistem rekomendasi yang akurat sangat sulit dibuat

### **Prediksi**
"""

df1 = pd.read_csv("/tmp/rating.csv")
df1.drop(df1[df1['rating'] == -1].index, inplace = True)
df1.drop(df1.index[50000:6337241], inplace=True)
df1

anime_df = dataset_new
 
# Mengambil sample user
user_id_ = df.user_id.sample(1).iloc[0]
anime_visited_by_user = df[df.user_id == user_id_]
 
# Operator bitwise (~), bisa diketahui di sini https://docs.python.org/3/reference/expressions.html 
anime_not_visited = anime_df[~anime_df['id'].isin(anime_visited_by_user.anime_id.values)]['id'] 
anime_not_visited = list(
    set(anime_not_visited)
    .intersection(set(anime_to_anime_encoded.keys()))
)
 
anime_not_visited = [[anime_to_anime_encoded.get(x)] for x in anime_not_visited]
user_encoder = user_to_user_encoded.get(user_id_)
user_anime_array = np.hstack(
    ([[user_encoder]] * len(anime_not_visited), anime_not_visited)
)

user_anime_array

ratings = model.predict(user_anime_array).flatten()
 
top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_anime_ids = [
    anime_encoded_to_anime.get(anime_not_visited[x][0]) for x in top_ratings_indices
]
 
print('Showing recommendations for users: {}'.format(user_id_))
print('===' * 9)
print('anime with high ratings from user')
print('----' * 8)
 
top_anime_user = (
    anime_visited_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .anime_id.values
)
 
anime_df_rows = anime_df[anime_df['id'].isin(top_anime_user)]
for row in anime_df_rows.itertuples():
    print(row.name, ':', row.genre)
 
print('----' * 8)
print('Top 10 anime recommendation')
print('----' * 8)
 
recommended_anime = anime_df[anime_df['id'].isin(recommended_anime_ids)]
for row in recommended_anime.itertuples():
    print(row.name, ':', row.genre)